# Release 2: Lexer (Tokenization)

> Follow CLAUDE.md - do not implement until user approves complete release plan.

## Goal

Build a lexer that converts source code into tokens. This is the first stage of the interpreter pipeline: source code → tokens → AST → execution.

The lexer will:
- Break source code into tokens (numbers, operators, keywords, identifiers, etc.)
- Track line and column position for each token (for error messages)
- Report errors for invalid characters or malformed tokens

No visualization yet - that's Release 3.

## Language Syntax (this release)

### Token Types

| Token Type | Examples | Description |
|------------|----------|-------------|
| NUMBER | `42`, `0`, `123` | Integer literals |
| STRING | `"hello"`, `"Hi Connor"` | String literals (double quotes) |
| IDENTIFIER | `x`, `count`, `myVar` | Variable and function names |
| KEYWORD | `let`, `if`, `print`, etc. | Reserved words |
| OPERATOR | `+`, `-`, `==`, etc. | Operators |
| PUNCTUATION | `(`, `)`, `{`, `}`, `,`, `;` | Structural characters |
| EOF | - | End of input |

### Keywords

```
let       - variable declaration
if        - conditional
else      - conditional branch
while     - loop
function  - function declaration
return    - return from function
true      - boolean literal
false     - boolean literal
print     - output to console
stop      - pause execution (debugger)
```

### Operators

| Operator | Description |
|----------|-------------|
| `+` | Addition |
| `-` | Subtraction / Negation |
| `*` | Multiplication |
| `/` | Division |
| `=` | Assignment |
| `==` | Equality |
| `!=` | Inequality |
| `<` | Less than |
| `>` | Greater than |
| `<=` | Less than or equal |
| `>=` | Greater than or equal |

### Punctuation

| Symbol | Description |
|--------|-------------|
| `(` `)` | Grouping, function calls |
| `{` `}` | Blocks |
| `,` | Separator |

### Newlines

Newlines separate statements (like Python). No semicolons needed.

### Comments

```
// This is a comment (ignored by lexer)
x = 5  // inline comment
```

### Whitespace

- Spaces and tabs are skipped (used only to separate tokens)
- Newlines are tracked for line numbers but otherwise treated as whitespace
- Multiple whitespace characters are collapsed

### Identifier Rules

- Start with letter or underscore: `a-z`, `A-Z`, `_`
- Followed by letters, digits, or underscores: `a-z`, `A-Z`, `0-9`, `_`
- Case-sensitive: `foo` and `Foo` are different
- Cannot be a keyword

### String Rules

- Enclosed in double quotes: `"hello"`
- Can contain spaces and punctuation
- Escape sequences (future): `\"`, `\\`, `\n`
- Unclosed string is an error

## Token Structure

Each token includes:
```javascript
{
  type: 'NUMBER',      // Token type
  value: 42,           // Parsed value (number, string, or raw text)
  line: 1,             // Line number (1-indexed)
  column: 5,           // Column number (1-indexed)
  raw: '42'            // Original source text
}
```

## Examples

### Input
```
let x = 42
print x + 1
```

### Output Tokens
```javascript
[
  { type: 'KEYWORD', value: 'let', line: 1, column: 1, raw: 'let' },
  { type: 'IDENTIFIER', value: 'x', line: 1, column: 5, raw: 'x' },
  { type: 'OPERATOR', value: '=', line: 1, column: 7, raw: '=' },
  { type: 'NUMBER', value: 42, line: 1, column: 9, raw: '42' },
  { type: 'KEYWORD', value: 'print', line: 2, column: 1, raw: 'print' },
  { type: 'IDENTIFIER', value: 'x', line: 2, column: 7, raw: 'x' },
  { type: 'OPERATOR', value: '+', line: 2, column: 9, raw: '+' },
  { type: 'NUMBER', value: 1, line: 2, column: 11, raw: '1' },
  { type: 'EOF', value: null, line: 2, column: 12, raw: '' }
]
```

## File Structure

```
src/
  lexer.js             # NEW - Lexer implementation
  main.js              # Updated - integrate lexer for testing
```

## Class Design

### Lexer (src/lexer.js)

```javascript
class Lexer {
  constructor(source) {
    this.source = source;
    this.pos = 0;           // Current position in source
    this.line = 1;          // Current line number
    this.column = 1;        // Current column number
    this.tokens = [];       // Collected tokens
  }

  // Main entry point
  tokenize() {
    // Returns { tokens: [...], errors: [...] }
  }

  // Internal methods
  peek()          // Look at current character without consuming
  advance()       // Consume current character and return it
  isAtEnd()       // Check if we've reached end of source
  skipWhitespace()
  readNumber()
  readString()
  readIdentifierOrKeyword()
  readOperator()
  makeToken(type, value, raw)
  makeError(message)
}

// Token type constants
const TokenType = {
  NUMBER: 'NUMBER',
  STRING: 'STRING',
  IDENTIFIER: 'IDENTIFIER',
  KEYWORD: 'KEYWORD',
  OPERATOR: 'OPERATOR',
  PUNCTUATION: 'PUNCTUATION',
  EOF: 'EOF'
};

// Keywords set
const KEYWORDS = new Set([
  'let', 'if', 'else', 'while', 'function',
  'return', 'true', 'false', 'print', 'stop'
]);
```

## UI Integration

Update main.js to show lexer output when Run is clicked:
- Tokenize the code in the editor
- Display tokens in the output pane (formatted for readability)
- Display any lexer errors

### Output Format

Success:
```
Tokens:
  1:1   KEYWORD     let
  1:5   IDENTIFIER  x
  1:7   OPERATOR    =
  1:9   NUMBER      42
  2:1   KEYWORD     print
  ...
```

Error:
```
Error at line 1, column 10: Unterminated string
```

## Error Handling

The lexer should collect errors but continue tokenizing when possible (error recovery). This helps show multiple errors at once.

| Error | Example | Message |
|-------|---------|---------|
| Invalid character | `x @ y` | `Invalid character '@'` |
| Unterminated string | `"hello` | `Unterminated string` |

## Playwright Tests

### Positive Tests

1. **"lexer tokenizes numbers"**
   - Input: `42`
   - Verify: NUMBER token with value 42

2. **"lexer tokenizes operators"**
   - Input: `+ - * / = == != < > <= >=`
   - Verify: correct OPERATOR tokens

3. **"lexer tokenizes keywords"**
   - Input: `let if else while function return true false print stop`
   - Verify: correct KEYWORD tokens

4. **"lexer tokenizes identifiers"**
   - Input: `x foo myVar _private count123`
   - Verify: correct IDENTIFIER tokens

5. **"lexer tokenizes strings"**
   - Input: `"hello" "Hi Connor"`
   - Verify: STRING tokens with correct values

6. **"lexer tokenizes punctuation"**
   - Input: `( ) { } ,`
   - Verify: correct PUNCTUATION tokens

7. **"lexer tracks line and column"**
   - Input: `let x\nprint x`
   - Verify: tokens have correct line/column positions

8. **"lexer skips comments"**
   - Input: `x = 5 // this is a comment\nprint x`
   - Verify: comment text not in tokens

9. **"lexer handles mixed input"**
   - Input: `let count = 0\nwhile (count < 10) {\n  print count\n}`
   - Verify: all tokens correct (KEYWORD, IDENTIFIER, OPERATOR, NUMBER, PUNCTUATION)

10. **"lexer tokenizes expression"**
    - Input: `(2 + 3) * 4`
    - Verify: correct sequence of tokens

### Negative Tests

11. **"lexer reports invalid character"**
    - Input: `x @ y`
    - Verify: error message contains "Invalid character '@'"

12. **"lexer reports unterminated string"**
    - Input: `"hello`
    - Verify: error message contains "Unterminated string"

13. **"lexer reports multiple errors"**
    - Input: `x @ y $ z`
    - Verify: multiple error messages

## Deliverables

- [x] src/lexer.js with Lexer class
- [x] TokenType constants and KEYWORDS set
- [x] Update src/main.js to use lexer
- [x] tests/lexer.spec.js with 13 tests
- [x] All tests passing (20 total: 7 UI + 13 lexer)

## Status

**COMPLETE**
